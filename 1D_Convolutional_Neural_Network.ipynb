{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1 - Creating 1D-Convolutional Layer"
      ],
      "metadata": {
        "id": "IE6i3RGtfTyN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI74eFw3eCbD"
      },
      "outputs": [],
      "source": [
        "class SimpleConv1d():\n",
        "\n",
        "    def forward(self, x, w, b):\n",
        "      a = []\n",
        "      for i in range(len(w) - 1):\n",
        "          a.append((x[i:i+len(w)] @ w) + b[0])\n",
        "      return np.array(a)\n",
        "\n",
        "    def backward(self, x, w, da):\n",
        "        db = np.sum(da)\n",
        "        dw = []\n",
        "        for i in range(len(w)):\n",
        "            dw.append(da @ x[i:i+len(da)])\n",
        "        dw = np.array(dw)\n",
        "        dx = []\n",
        "        new_w = np.insert(w[::-1], 0, 0)\n",
        "        new_w = np.append(new_w, 0)\n",
        "        for i in range(len(new_w)-1):\n",
        "            dx.append(new_w[i:i+len(da)] @ da)\n",
        "        dx = np.array(dx[::-1])\n",
        "        return db, dw, dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 2 - Output Size calculator"
      ],
      "metadata": {
        "id": "Kp5TjPTxqHJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_size_calculator(n_in, P, F, S):\n",
        "  n_out = int((n_in + 2*P - F) / S + 1)\n",
        "  return n_out"
      ],
      "metadata": {
        "id": "1k5NkdCHqGR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3 - Experimentation of the 1D CL"
      ],
      "metadata": {
        "id": "_D93clCBq4Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1,2,3,4])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])"
      ],
      "metadata": {
        "id": "EiYHMxlwq3wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([35, 50])"
      ],
      "metadata": {
        "id": "f40OuWZAr1iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_a = np.array([10, 20])"
      ],
      "metadata": {
        "id": "Vr1B4E8xsDOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_b = np.array([30])\n",
        "delta_w = np.array([50, 80, 110])\n",
        "delta_x = np.array([30, 110, 170, 140])"
      ],
      "metadata": {
        "id": "StOC8HjpsPdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([1,2,3,4])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "da = np.array([10, 20])\n"
      ],
      "metadata": {
        "id": "htO01jIIt8Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SC1D = SimpleConv1d()\n",
        "db, dw, dx = SC1D.backward(x, w, da)\n",
        "print(SC1D.forward(x, w, b))\n",
        "print(db)\n",
        "print(dw)\n",
        "print(dx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoR0_YKo9Ah7",
        "outputId": "ba31880e-66ac-4c6f-d127-adcf47dd5966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35 50]\n",
            "30\n",
            "[ 50  80 110]\n",
            "[ 30 110 170 140]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 4 - Creating a one dimensional convo. layerclass that doesn't limitthe number of channels"
      ],
      "metadata": {
        "id": "X_cX3J2i_N2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape (2, 4), (Number of input channels, number of features).\n",
        "w = np.ones((3, 2, 3))\n",
        "b = np.array([1, 2, 3])"
      ],
      "metadata": {
        "id": "eG41l7M5_NUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[16, 22], [17, 23], [18, 24]])"
      ],
      "metadata": {
        "id": "Z7SGayFVAIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self,x,y, batch_size = 20, seed=0):\n",
        "      self.batch_size = batch_size\n",
        "      np.random.seed(seed)\n",
        "      shuffle_index = np.random.permutation(np.arange(x.shape[0]))\n",
        "      self._x = x[shuffle_index]\n",
        "      self._y = y[shuffle_index]\n",
        "      self._stop = np.cell(x.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._x[p0:p1], self.y[p0:p1]\n",
        "    \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.x[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "RwqcUMPrAXnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma =sigma\n",
        "    \n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    \n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B"
      ],
      "metadata": {
        "id": "lxwBKLSbXoGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    \n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW / len(layer.Z)\n",
        "        layer.B -= self.lr * layer.dB / len(layer.Z)\n",
        "        return layer"
      ],
      "metadata": {
        "id": "HPLpr1n8-GYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "\n",
        "    def update(self,layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr *np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr *np.sqrt(1/self.HB) * layer.dB"
      ],
      "metadata": {
        "id": "4sB_UEge-vtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_size_calculator(n_in, F, P=0, S=1):\n",
        "    n_out = int((n_in + 2*P - F) / S + 1)\n",
        "    return n_out"
      ],
      "metadata": {
        "id": "8xm2xxd1AhuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 5 - Implementing Padding"
      ],
      "metadata": {
        "id": "O9o7STtXNS6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1d:\n",
        "\n",
        "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0):\n",
        "        self.b_size = b_size\n",
        "        self.optimizer = optimizer\n",
        "        self.pa = pa\n",
        "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
        "        self.B = initializer.B(n_out_channels)\n",
        "        self.n_in_channels = n_in_channels\n",
        "        self.n_out_channels = n_out_channels\n",
        "        self.n_out = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.n_in = X.shape[-1]\n",
        "        self.n_out = output_size_calculator(self.n_in, self.b_size, self.pa)\n",
        "        X = X.reshape(self.n_in_channels, self.n_in)\n",
        "        self.X = np.pad(X, ((0,0), ((self.b_size-1), 0)))\n",
        "        self.X1 = np.zeros((self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
        "        for i in range(self.b_size):\n",
        "            self.X1[:, i] = np.roll(self.X, -1, axis=-1)\n",
        "        A = np.sum(self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]*self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.B.reshape(-1,1)\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa, np.newaxis]), axis=-1)\n",
        "        self.dB = np.sum(dA, axis=1)\n",
        "        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\n",
        "        self.dA1 = np.zeros((self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
        "        for i in range(self.b_size):\n",
        "          self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n",
        "        dX = np.sum(self.W@self.dA1, axis=0)\n",
        "        self.optimizer.update(self)\n",
        "        print(\"dW:\", self.dW)\n",
        "        print(\"dB:\", self.dB)\n",
        "        print(\"dX:\", dX)\n",
        "        return dX"
      ],
      "metadata": {
        "id": "o8WEWtvXBxx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 6 - Response to Mini batch"
      ],
      "metadata": {
        "id": "pFjlvGpoNcS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "test = Conv1d(b_size=3, initializer=SimpleInitializer(0.01), optimizer=AdaGrad(0.01), n_in_channels=2, n_out_channels=3, pa=0)"
      ],
      "metadata": {
        "id": "pi30tmo5Hzj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
        "test.W = np.ones((3, 2, 3), dtype=float)\n",
        "test.B = np.array([1, 2, 3], dtype=float)"
      ],
      "metadata": {
        "id": "dt63AQPjI-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing = test.forward(x)\n",
        "testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOS24IGlJaRN",
        "outputId": "aec11d51-371d-4d85-b8bb-13dd5936a64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16., 22.],\n",
              "       [17., 23.],\n",
              "       [18., 24.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.backward(testing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsuHoyz8JwYg",
        "outputId": "fcff600e-91b4-4e77-d830-6d71e977ff44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW: [[[ 98.  98.  98.]\n",
            "  [136. 136. 136.]]\n",
            "\n",
            " [[103. 103. 103.]\n",
            "  [143. 143. 143.]]\n",
            "\n",
            " [[108. 108. 108.]\n",
            "  [150. 150. 150.]]]\n",
            "dB: [38. 40. 42.]\n",
            "dX: [[ 51. 120. 120.  69.]\n",
            " [ 51. 120. 120.  69.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 51., 120., 120.,  69.],\n",
              "       [ 51., 120., 120.,  69.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 7 - Arbitrary Number of Strides"
      ],
      "metadata": {
        "id": "jYC4LEvBMsIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1d_Arbitrary_strides:\n",
        "\n",
        "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n",
        "        self.b_size = b_size\n",
        "        self.optimizer = optimizer\n",
        "        self.pa = pa\n",
        "        self.stride = stride\n",
        "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
        "        self.B = initializer.B(n_out_channels)\n",
        "        self.n_in_channels = n_in_channels\n",
        "        self.n_out_channels = n_out_channels\n",
        "        self.n_out = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.n_in = X.shape[-1]\n",
        "        self.n_out = output_size_calculator(self.n_in, self.b_size, self.pa, self.stride)\n",
        "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
        "        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
        "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
        "        for i in range(self.b_size):\n",
        "            self.X1[:, :, i] = np.roll(self.X, -1, axis=-1)\n",
        "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
        "        self.dB = np.sum(dA, axis=(0, -1))\n",
        "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
        "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
        "        for i in range(self.b_size):\n",
        "          self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
        "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
        "        self.optimizer.update(self)\n",
        "        return dX"
      ],
      "metadata": {
        "id": "fZfzLSr3MrFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 8 - Learning and Estimation"
      ],
      "metadata": {
        "id": "MUw_cGQoM4mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "x_train = x_train.astype(np.float)\n",
        "x_test = x_test.astype(np.float)\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "print(y_train.shape) \n",
        "print(y_train_one_hot.shape)\n",
        "print(y_train_one_hot.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZKAHSS_M37f",
        "outputId": "62661170-e806-4bbf-f160-6a9472b6a1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-c9d2fc207084>:18: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x_train = x_train.astype(np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-c9d2fc207084>:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x_test = x_test.astype(np.float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.2)\n",
        "print(\"x_train.shape =\", x_train_.shape)\n",
        "print(\"x_val.shape =\", x_val.shape)\n",
        "print(\"y_train.shape =\", y_train_.shape)\n",
        "print(\"y_val.shape =\", y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loQ8vPzqXdFk",
        "outputId": "829f36f5-f58b-4885-af2f-32984be25af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape = (48000, 784)\n",
            "x_val.shape = (12000, 784)\n",
            "y_train.shape = (48000, 10)\n",
            "y_val.shape = (12000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "o7JJgO7kS7fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_mini_batch = GetMiniBatch(x_train, y_train, batch_size=20)\n",
        "for mini_x_train, mini_y_train in get_mini_batch:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7A43Blg6ZDZ",
        "outputId": "fe2ea4e7-844a-4e82-bbba-900e1f784c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-01a04e45f93e>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = Conv1d_Arbitrary_strides(b_size=3, initializer=SimpleInitializer(0.01), optimizer=SGD(0.01), n_in_channels=1, n_out_channels=1, pa=1)\n",
        "testing2 = test2.forward(mini_x_train)\n",
        "print(testing2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc7Vi9L2aga-",
        "outputId": "1c296d06-004c-4cac-c713-2d0b7076af4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 1, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Sigmoid:\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Tanh:\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - np.tanh(self.A)**2)\n",
        "\n",
        "class Softmax:\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    \n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    \n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
        "\n",
        "class FC:\n",
        "    \n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.B\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ        \n",
        "\n",
        "class XavierInitializer:\n",
        "    \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class HeInitializer:\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    \n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "\n",
        "    def update(self,layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr *np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr *np.sqrt(1/self.HB) * layer.dB"
      ],
      "metadata": {
        "id": "4wpeYE3gbc_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scratch Convolutional Neural Net. Classifier"
      ],
      "metadata": {
        "id": "fhFikNeqmQuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class ScratchCNNClassifier:\n",
        "    \n",
        "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose  \n",
        "        self.batch_size = batch_size \n",
        "        self.n_features = n_features \n",
        "        self.n_nodes2 = n_nodes2 \n",
        "        self.n_output = n_output \n",
        "        self.Activater = Activater\n",
        "        if Activater == Sigmoid or Activater == Tanh:\n",
        "            self.Initializer = XavierInitializer\n",
        "        elif Activater == ReLU:\n",
        "            self.Initializer = HeInitializer\n",
        "        self.Optimizer = Optimizer\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        self.val_enable = False\n",
        "        if X_val is not None:\n",
        "            self.val_enable = True\n",
        "        self.Conv1d_Arbitrary_Strides = Conv1d_Arbitrary_strides(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
        "        self.Conv1d_Arbitrary_Strides.n_out = output_size_calculator(X.shape[-1], self.Conv1d_Arbitrary_Strides.b_size, self.Conv1d_Arbitrary_Strides.pa, self.Conv1d_Arbitrary_Strides.stride)\n",
        "        self.activation1 = self.Activater()\n",
        "        self.FC2 = FC(1*self.Conv1d_Arbitrary_Strides.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n",
        "        self.activation2 = self.Activater()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n",
        "        self.activation3 = Softmax()\n",
        "        \n",
        "        self.loss = []\n",
        "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
        "        \n",
        "        for _ in range(self.num_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            self.iter = len(get_mini_batch)\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                self.forward_propagation(mini_X)\n",
        "                self.back_propagation(mini_X, mini_y)\n",
        "                self.loss.append(self.activation3.loss)\n",
        "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward_propagation(X), axis=1)\n",
        "    \n",
        "    def forward_propagation(self, X):\n",
        "        A1 = self.Conv1d_Arbitrary_Strides.forward(X)\n",
        "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        return Z3\n",
        "        \n",
        "    def back_propagation(self, X, y_true):\n",
        "        dA3 = self.activation3.backward(y_true) \n",
        "        dZ2 = self.FC3.backward(dA3)\n",
        "        dA2 = self.activation2.backward(dZ2)\n",
        "        dZ1 = self.FC2.backward(dA2)\n",
        "        dA1 = self.activation1.backward(dZ1)\n",
        "        dA1 = dA1[:, np.newaxis]\n",
        "        dZ0 = self.Conv1d_Arbitrary_strides.backward(dA1) "
      ],
      "metadata": {
        "id": "Apkyuto4kTUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test3 = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\n",
        "test3.fit(x_train_, y_train_)"
      ],
      "metadata": {
        "id": "DWTI4Mj2wwR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99526a7a-5d67-4796-807e-e165c39744ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-01a04e45f93e>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = test3.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "b_zh5VVgygoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9970510a-f226-4270-89b8-6e598795910b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9765"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}