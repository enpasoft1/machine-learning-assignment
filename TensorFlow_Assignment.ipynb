{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1 - Looking Back at Scratch\n",
        "\n",
        "Initialization of the weights and Bias\n",
        "\n",
        "*   Epoch loop was needed\n",
        "*   Number of nodes were determined and activation for each layer was performed\n",
        "*   Training data was the mini-batch size\n",
        "*   Assigned the linearly combined value to the activation function and passed the results to the next later (Forward Propagation)\n",
        "*Computed the Loss Function\n",
        "*Updated the weights and Biases (Error Back Propagation)\n",
        "*Predicted with the validation data using the learned parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ymJo6JT7Js4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Problem 2 - Consider the correspondence between scratch and TensorFlow"
      ],
      "metadata": {
        "id": "uRAVSo9G9QRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u0AZs8x2_MMJ",
        "outputId": "59ac5513-5735-4f97-b6e2-f6a7e2e5d587"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "dvlclChP85H6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "7m4C39bx-bhE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    #Declaration of weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    # tf.add and + are equivalent\n",
        "    \n",
        "    return layer_output"
      ],
      "metadata": {
        "id": "pVY_oy3u-5Il"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Binar classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Load dataset\n",
        "dataset_path = \"./Iris/Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "#Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "#Convert labels to numbers\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
        "\n",
        "#Further split into train and Validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "#Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
      ],
      "metadata": {
        "id": "rCShCHGQAElY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3 - Create a model of Iris using all three types of objective variables"
      ],
      "metadata": {
        "id": "sE3D2rb_ELD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "dataset_path = \"./Iris/Iris.csv\"\n",
        "df_iris = pd.read_csv(dataset_path)\n",
        "\n",
        "#Condition extraction from data frame\n",
        "df_iris = df_iris[(df_iris[\"Species\"] == \"Iris-setosa\") \n",
        "                  |(df_iris[\"Species\"] == \"Iris-versicolor\")\n",
        "                  |(df_iris[\"Species\"] == \"Iris-virginica\")\n",
        "                  ]\n",
        "\n",
        "y = df_iris[\"Species\"]\n",
        "X = df_iris.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "#Convert labels to numbers\n",
        "y[y =='Iris-setosa'] = 0\n",
        "y[y =='Iris-versicolor'] = 1\n",
        "y[y =='Iris-virginica'] = 2\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y = enc.fit_transform(y[:, np.newaxis])\n",
        "\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Further split into train and Validation\n",
        "X_train, X_val, y_train, y_val = \\\n",
        "train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "6ZqRVEGjDyoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('y.shape:', y.shape)\n",
        "print('X.shape:', X.shape)"
      ],
      "metadata": {
        "id": "wk_9k4g3IRxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the Train Data\n",
        "import seaborn as sns\n",
        "sns.pairplot(df_iris,hue='Species');"
      ],
      "metadata": {
        "id": "qcvO1pYLIScI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure another set of Hyperparameter\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "                         labels=Y, logits=logits)\n",
        "                        )\n",
        "\n",
        "#Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(tf.sparse_softmax(logits),1))\n",
        "\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        \n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            \n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run(\n",
        "                [loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y}\n",
        "            )\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "\n",
        "        val_loss, val_acc = sess.run(\n",
        "            [loss_op, accuracy], feed_dict={X: X_val, Y: y_val}\n",
        "        )\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\"\n",
        "              .format(epoch, loss, val_loss, acc, val_acc))\n",
        "    \n",
        "    \n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "kYSWbr4hLJul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 4 - Create a model of House Prices\n",
        "Create a model using House Prices, a data set for regression problems"
      ],
      "metadata": {
        "id": "wXEnE98iZAW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "dataset_path = \"house_prices_advanced_regression_technique/train.csv\"\n",
        "df_house = pd.read_csv(dataset_path)\n",
        "\n",
        "#Extracting Conditions from the data frame\n",
        "y = df_house[['SalePrice']]\n",
        "X = df_house[['GrLivArea', 'YearBuilt']]\n",
        "y = np.array(np.log1p(y))\n",
        "X = np.array(np.log1p(X))\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Further split into train and Validation\n",
        "X_train, X_val, y_train, y_val = \\\n",
        "train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "ygHz7DRJY-ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('y.shape:', y.shape)\n",
        "print('X.shape:', X.shape)"
      ],
      "metadata": {
        "id": "YkqrZpdkgyCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the Train Data\n",
        "import matplotlib as plt\n",
        "\n",
        "plt.rcParams[\"font.size\"] = 20\n",
        "fig,ax = plt.subplots(1,2,sharey=True,figsize=(16, 5))\n",
        "\n",
        "ax[0].scatter(X_train[:,0],y_train)\n",
        "ax[1].scatter(X_train[:,1],y_train);"
      ],
      "metadata": {
        "id": "wiqVSmhVg4Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_net(x):\n",
        "\n",
        "    # Declaring the weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "\n",
        "    #tf.add and + are equivalent\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "\n",
        "    return layer_output"
      ],
      "metadata": {
        "id": "Ii3r8H-RijYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure another set of Hyperparameter\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(tf.float, [None, n_input])\n",
        "Y = tf.placeholder(tf.float, [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.square(logits=logits - Y))\n",
        "\n",
        "#Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Index Value Calculation\n",
        "mean_square_error = tf.reduce_mean(tf.square(logits=logits - Y))\n",
        "\n",
        "#Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_mse = 0\n",
        "        \n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            \n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "\n",
        "            loss, mse = sess.run([loss_op, mean_square_error], \n",
        "                                 feed_dict={X: mini_batch_x, Y: mini_batch_y}\n",
        "            )\n",
        "            total_loss += loss\n",
        "            total_mse += mse\n",
        "\n",
        "        total_loss /= n_samples\n",
        "        total_mse /= n_samples\n",
        "\n",
        "        val_loss, val_mse = sess.run([loss_op, mean_square_error], \n",
        "                                     feed_dict={X: X_val, Y: y_val}\n",
        "        )\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, mse : {:.3f}, val_mse : {:.3f}\"\n",
        "              .format(epoch, loss, val_loss, mse, val_mse))\n",
        "        \n",
        "    test_mse = sess.run(mean_square_error, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_mse : {:.3f}\".format(test_mse))\n"
      ],
      "metadata": {
        "id": "lN_wuDyblLtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 5 - Create a model of MNIST\n",
        "Create a model to classify the MNIST used in neural network scratches.\n",
        "\n",
        "*It is similar to the previous Iris in that it is classified into 3 or more classes. The difference is that the input is an image.\n",
        "\n",
        "*Aim to reproduce the model mounted by scratch."
      ],
      "metadata": {
        "id": "EMeohkBDqhw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the MNIST Dataset\n",
        "from keras.datasets import mnist\n",
        "(x,y), (X_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "dRZuX1u3qhAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the Data imputed\n",
        "print(X.shape)\n",
        "print(X.shape)\n",
        "print(X[0].dtype)"
      ],
      "metadata": {
        "id": "jGE3p2HosD8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Smooting\n",
        "X = X.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1,784)\n",
        "print(X.shape) \n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "QL15sxzJsXvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Type Conversion, Normalization\n",
        "X = X.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X /= 255\n",
        "X_test /= 255\n",
        "print(x.max())\n",
        "print(x.min())"
      ],
      "metadata": {
        "id": "eko91luLsvlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
        "y_test = enc.transform(y_test[:, np.newaxis])\n",
        "print(y.shape)\n",
        "print(y_one_hot.shape)\n",
        "print(y_one_hot.dtype)"
      ],
      "metadata": {
        "id": "lDMHm0eYtVgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "id": "7V-rN0j1wK1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lenet(x):\n",
        "    \"\"\"\n",
        "    CNN\n",
        "    \"\"\"\n",
        "    #Declaration of weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([5,5,1,6])),\n",
        "        'w2': tf.Variable(tf.random_normal([5,5,6,16])),\n",
        "        'w3': tf.Variable(tf.random_normal([7*7*16, 120])),\n",
        "        'w4': tf.Variable(tf.random_normal([120,84])),\n",
        "        'w5': tf.Variable(tf.random_normal([84, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([6])),\n",
        "        'b2': tf.Variable(tf.random_normal([16])),\n",
        "        'b3': tf.Variable(tf.random_normal([120])),\n",
        "        'b4': tf.Variable(tf.random_normal([84])),\n",
        "        'b5': tf.Variable(tf.random_normal([84, n_classes]))\n",
        "    }\n",
        "\n",
        "    X = tf.reshape(X, [-1, 28, 28, 1])\n",
        "    conve_1 = tf.add(tf.nn.conv2d(X, weights['w1'],strides=[1,1,1,1],\n",
        "                                 padding='SAME'),\n",
        "                    biases['b1'])\n",
        "    \n",
        "    conve_1 = tf.nn.relu(conve_1)\n",
        "\n",
        "    poolin_1 = tf.nn.pool(conve_1, window_shape=[2,2],strides=[2,2],\n",
        "                          pooling_type='MAX', padding='VALID')\n",
        "    \n",
        "    conve_2 = tf.add(tf.nn.conv2d(poolin_1, weights['w2'],strides=[1,1,1,1],\n",
        "                                  padding='SAME'),\n",
        "                     biases['b2'])\n",
        "    \n",
        "    conve_2 = tf.nn.relu(conve_2)\n",
        "\n",
        "    poolin_2 = tf.nn.pool(conve_2, window_shape=[2,2],strides=[2,2],\n",
        "                          pooling_type='MAX', padding='VALID')\n",
        "    \n",
        "    X_reshape = tf.reshape(poolin_2, [-1,7*7*16])\n",
        "    layer_1 = tf.add(tf.matmul(X_reshape, weights['w3']), biases['b3'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(X_reshape, weights['w4']), biases['b4'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # tf.add and + are equivalent\n",
        "    layer_output = tf.matmul(layer_2, weights['w5']) + biases['b5']\n",
        "    \n",
        "    return layer_output"
      ],
      "metadata": {
        "id": "ksu_yOzpzgWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure another set of Hyperparameter\n",
        "learning_rate = 0.01\n",
        "batch_size = 200\n",
        "num_epochs = 30\n",
        "\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "#Read network structure                              \n",
        "logits = lenet(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "                          labels=Y, logits=logits - Y)\n",
        "                          )\n",
        "\n",
        "#Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(tf.nn_softmax(logits),1))\n",
        "\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        \n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            \n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "\n",
        "            loss, acc = sess.run(\n",
        "                [loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "\n",
        "        val_loss, val_acc = sess.run(\n",
        "            [loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\"\n",
        "              .format(epoch, loss, val_loss, acc, val_acc))\n",
        "        \n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
      ],
      "metadata": {
        "id": "dQmVky5r4rAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}